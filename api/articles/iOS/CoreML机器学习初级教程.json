{"title":"CoreML机器学习初级教程","slug":"iOS/CoreML机器学习初级教程","date":"2017-06-24T22:10:44.000Z","updated":"2017-06-24T22:24:54.000Z","comments":true,"path":"api/articles/iOS/CoreML机器学习初级教程.json","excerpt":null,"covers":["http://upload-images.jianshu.io/upload_images/861914-5425960c41207b82.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240","http://upload-images.jianshu.io/upload_images/861914-e00a802f64f0ae13.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240","http://upload-images.jianshu.io/upload_images/861914-1268b5b918fccf07.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240","http://upload-images.jianshu.io/upload_images/861914-bb5bec38118f91ff.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240","http://upload-images.jianshu.io/upload_images/861914-e194482f995324a1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"],"content":"<script src=\"/assets/js/APlayer.min.js\"> </script><h3 id=\"资源\"><a href=\"#资源\" class=\"headerlink\" title=\"资源\"></a>资源</h3><p><a href=\"https://developer.apple.com/documentation/coreml\" target=\"_blank\" rel=\"noopener\">Core ML开发文档</a><br><a href=\"https://developer.apple.com/machine-learning/\" target=\"_blank\" rel=\"noopener\">ML模型资源页面</a><br>在Working with Models中包含有几个常用的模型模板，例如用于在图片中检测物体——树、动物、人等等。<br><a href=\"https://developer.apple.com/documentation/coreml/integrating_a_core_ml_model_into_your_app\" target=\"_blank\" rel=\"noopener\">Integrating a Core ML Model into Your App</a></p>\n<p><a href=\"https://docs-assets.developer.apple.com/published/51ff0c1668/IntegratingaCoreMLModelintoYourApp.zip\" target=\"_blank\" rel=\"noopener\">官方Core ML文档示例 App</a><br>MarsHabitatPricePredictor 模型的输入只是数字，因此代码直接使用生成的 MarsHabitatPricer 方法和属性，而不是将模型包装在 Vision 模型中。每次都改一下参数，很容易看出模型只是一个线性回归：<br>137 <em> solarPanels + 653.50 </em> greenHouses + 5854 * acres</p>\n<h3 id=\"配置-：将-Core-ML-模型集成到你的-App\"><a href=\"#配置-：将-Core-ML-模型集成到你的-App\" class=\"headerlink\" title=\"配置 ：将 Core ML 模型集成到你的 App\"></a>配置 ：将 Core ML 模型集成到你的 App</h3><p>本教程使用 Places205-GoogLeNet 模型，可以从苹果的<a href=\"https://developer.apple.com/machine-learning/\" target=\"_blank\" rel=\"noopener\">ML</a>页面下载。往下滑找到 Working with Models，下载第一个。还在这个页面，注意一下其它三个模型，它们都用于在图片中检测物体——树、动物、人等等。</p>\n<blockquote>\n<p>注意：如果你有一个训练过的模型，并且是使用受支持的机器学习工具训练的，例如 Caffe、Keras 或 scikit-learn，Converting Trained Models to Core ML 介绍了如何将其转换为 Core ML 格式。</p>\n</blockquote>\n<ol>\n<li>添加模型<br>下载 GoogLeNetPlaces.mlmodel 后，把它从 Finder 拖到项目导航器的 Resources 组里：<br><img src=\"http://upload-images.jianshu.io/upload_images/861914-5425960c41207b82.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"\"></li>\n<li>生成模型类<br>选择该文件，然后等一会儿。Xcode 生成了模型类后会显示一个箭头：<br><img src=\"http://upload-images.jianshu.io/upload_images/861914-e00a802f64f0ae13.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"\"></li>\n<li>查看模型类<br>点击箭头，查看生成的类：<br><img src=\"http://upload-images.jianshu.io/upload_images/861914-1268b5b918fccf07.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"\"><br>三个类：<br><code>GoogLeNetPlaces</code>: 主类，包含一个 model 属性和两个 prediction 方法<br><code>GoogLeNetPlacesInput</code>: 输入类,包含一个 CVPixelBuffer 类型的 sceneImage 属性，Vision 框架会负责把我们熟悉的图片格式转换成正确的输入类型。<br><code>GoogLeNetPlacesOutput</code>：输出属性,Vision 框架会将 <code>GoogLeNetPlacesOutput</code> 属性转换为自己的 <code>results</code> 类型.</li>\n</ol>\n<h3 id=\"实现Vision工作流程\"><a href=\"#实现Vision工作流程\" class=\"headerlink\" title=\"实现Vision工作流程\"></a>实现Vision工作流程</h3><p>标准的 Vision 工作流程是创建模型，创建一或多个请求，然后创建并运行请求处理程序。<br>并管理对 <code>prediction</code>方法的调用，所以在所有生成的代码中，我们只会使用 <code>model</code> 属性。</p>\n<ol>\n<li><p>创建模型：在 <code>Vision Model</code> 中包装 <code>Core ML Model</code><br>CoreML模型 是用于 Vision 请求的 Core ML 模型的容器<br>打开 ViewController.swift，并在 import UIKit 下面 import 两个框架：</p>\n<figure class=\"highlight swift\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> CoreML</span><br><span class=\"line\"><span class=\"keyword\">import</span> Vision</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>创建<code>VNCoreMLRequest</code>图像分析请求<br><code>VNCoreMLRequest</code> 是一个图像分析请求，它使用 Core ML 模型来完成工作。它的 completion handler 接收 request 和 error 对象。<br>Core ML 模型<code>GoogLeNetPlaces</code> 是一个分类器，因为它仅预测一个特征：图像的场景分类。这时request.results 是 <code>VNClassificationObservation</code> 对象数组。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">// 创建一个带有 completion handler 的 Vision 请求</span><br><span class=\"line\">let request = VNCoreMLRequest(model: model) &#123; [weak self] request, error in</span><br><span class=\"line\">    guard let results = request.results as? [VNClassificationObservation],</span><br><span class=\"line\">    let topResult = results.first else &#123;</span><br><span class=\"line\">    fatalError(&quot;unexpected result type from VNCoreMLRequest&quot;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">// 在主线程上更新 UI</span><br><span class=\"line\">let article = (self?.vowels.contains(topResult.identifier.first!))! ? &quot;an&quot; : &quot;a&quot;</span><br><span class=\"line\">    DispatchQueue.main.async &#123; [weak self] in</span><br><span class=\"line\">        self?.answerLabel.text = &quot;\\(Int(topResult.confidence * 100))% it&apos;s \\(article) \\(topResult.identifier)&quot;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>VNClassificationObservation 有两个属性：identifier - 一个 String，以及 confidence - 介于0和1之间的数字，这个数字是是分类正确的概率。使用对象检测模型时，你可能只会看到那些 confidence 大于某个阈值的对象，例如 30％ 的阈值。<br>然后取第一个结果，它会具有最高的 confidence 值，然后根据 identifier 的首字母把不定冠词设置为“a”或“an”。最后，dispatch 回到主线程来更新 label。你很快会明白分类工作为什么不在主线程，因为它会很慢。</p>\n<h3 id=\"创建并运行VNImageRequestHandler请求处理程序\"><a href=\"#创建并运行VNImageRequestHandler请求处理程序\" class=\"headerlink\" title=\"创建并运行VNImageRequestHandler请求处理程序\"></a>创建并运行VNImageRequestHandler请求处理程序</h3><p><code>VNImageRequestHandler</code> 是标准的 Vision 框架请求处理程序；不特定于 Core ML 模型。给它 image 作为 detectScene(image:) 的参数。然后调用它的 perform 方法来运行处理程序，传入请求数组。在这个例子里，我们只有一个请求。<br>把下面几行添加到 <code>detectScene(image:)</code> 的末尾：<br><figure class=\"highlight swift\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 在主线程上运行 Core ML GoogLeNetPlaces 分类器</span></span><br><span class=\"line\"><span class=\"keyword\">let</span> handler = <span class=\"type\">VNImageRequestHandler</span>(ciImage: image)</span><br><span class=\"line\"><span class=\"type\">DispatchQueue</span>.global(qos: .userInteractive).async &#123;</span><br><span class=\"line\">    <span class=\"keyword\">do</span> &#123;</span><br><span class=\"line\">        <span class=\"keyword\">try</span> handler.perform([request])</span><br><span class=\"line\">    &#125; <span class=\"keyword\">catch</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(error)</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"使用模型来自动识别场景\"><a href=\"#使用模型来自动识别场景\" class=\"headerlink\" title=\"使用模型来自动识别场景\"></a>使用模型来自动识别场景</h3><p>在两个地方调用 <code>detectScene(image:)</code><br>把下面几行添加到 viewDidLoad() 的末端和 imagePickerController(_:didFinishPickingMediaWithInfo:) 的末端：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">guard let ciImage = CIImage(image: image) else &#123;</span><br><span class=\"line\">    fatalError(&quot;couldn&apos;t convert UIImage to CIImage&quot;)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">detectScene(image: ciImage)</span><br></pre></td></tr></table></figure></p>\n<p>现在构建并运行。</p>\n<ol>\n<li>场景一:<br>机器识别出了50%的概率是摩天大厦<br><img src=\"http://upload-images.jianshu.io/upload_images/861914-bb5bec38118f91ff.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"\"></li>\n<li>场景二<br>机器人识别出了75%的概率是水族池<br><img src=\"http://upload-images.jianshu.io/upload_images/861914-e194482f995324a1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" alt=\"\"></li>\n</ol>\n<h3 id=\"什么是深度学习\"><a href=\"#什么是深度学习\" class=\"headerlink\" title=\"什么是深度学习\"></a>什么是深度学习</h3><p>自20世纪50年代以来，AI 研究人员开发了许多机器学习方法。苹果的 Core ML 框架支持神经网络、树组合、支持向量机、广义线性模型、特征工程和流水线模型。但是，神经网络最近已经取得了很多极为神奇的成功，开始于 2012 年谷歌使用 YouTube 视频训练 AI 来识别猫和人。仅仅五年后，谷歌正在赞助一场确定 5000 种植物和动物的比赛。像 Siri 和 Alexa 这样的 App 也存在它们自己的神经网络。<br>神经网络尝试用节点层来模拟人脑流程，并将节点层用不同的方式连接在一起。每增加一层都需要增加大量计算能力：Inception v3，一个对象识别模型，有48层以及大约2000万个参数。但计算基本上都是矩阵乘法，GPU 来处理会非常有效。GPU 成本的下降使我们能够创建多层深度神经网络，此为深度学习。<br>神经网络，circa 2016<br>神经网络需要大量的训练数据，这些训练数据理想化地代表了全部可能性。用户生成的数据爆炸性地产生也促成了机器学习的复兴。<br>训练模型意味着给神经网络提供训练数据，并让它计算公式，此公式组合输入参数以产生输出。训练是离线的，通常在具有多个 GPU 的机器上。<br>要使用这个模型，就给它新的输入，它就会计算输出：这叫做推论。推论仍然需要大量计算，以从新的输入计算输出。因为有了 Metal 这样的框架，现在可以在手持设备上进行这些计算。<br>在本教程的结尾你会发现，深度学习远非完美。真的很难建立具有代表性的训练数据，很容易就会过度训练模型，以至于它会过度重视一些古怪的特征。<br>苹果提供了什么？<br>苹果在 iOS 5 里引入了 NSLinguisticTagger 来分析自然语言。iOS 8 出了 Metal，提供了对设备 GPU 的底层访问。<br>去年，苹果在 Accelerate 框架添加了 Basic Neural Network Subroutines (BNNS)，使开发者可以构建用于推理（不是训练）的神经网络。<br>今年，苹果给了我们 Core ML 和 Vision！<br>Core ML 让我们更容易在 App 中使用训练过的模型。<br>Vision 让我们轻松访问苹果的模型，用于面部检测、面部特征点、文字、矩形、条形码和物体。<br>你还可以在 Vision 模型中包装任意的图像分析 Core ML 模型，我们在这篇教程中就干这个。由于这两个框架是基于 Metal 构建的，它们能在设备上高效运行，所以不需要把用户的数据发送到服务器。</p>\n","categories":[{"name":"iOS","path":"api/categories/iOS.json"},{"name":"API","path":"api/categories/API.json"}],"tags":[]}